{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled)\n",
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import theano\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "FIELD_SIZE = 5 * 300\n",
    "STRIDE = 1\n",
    "N_FILTERS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeData(text):\n",
    "    textList = list(text)\n",
    "    returnList = []\n",
    "    for item in textList[:1014]:\n",
    "        returnList.append(ord(item))\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validDocsDict = dict()\n",
    "fileList = os.listdir(\"BioMedProcessed\")\n",
    "for f in fileList:\n",
    "    validDocsDict.update(pickle.load(open(\"BioMedProcessed/\" + f, \"rb\")))\n",
    "\n",
    "#validDocsDict2 = dict()\n",
    "#fileList = os.listdir(\"PubMedProcessed\")\n",
    "#for f in fileList:\n",
    "#    validDocsDict2.update(pickle.load(open(\"PubMedProcessed/\" + f, \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "45002\n",
      "7804.54329226\n",
      "7997.28294031\n",
      "2250\n",
      "2250\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "documents = []\n",
    "testPubDocuments = []\n",
    "allDocuments = []\n",
    "labels = []\n",
    "testPubLabels = []\n",
    "concLengthTotal = 0\n",
    "discLengthTotal = 0\n",
    "concCount = 0\n",
    "discCount = 0\n",
    "charLength = 1014\n",
    "charList = []\n",
    "\n",
    "#combinedDicts = validDocsDict.copy()\n",
    "#combinedDicts.update(validDocsDict2.copy())\n",
    "\n",
    "for k in validDocsDict.keys():\n",
    "    if k.startswith(\"conclusion\") and len(validDocsDict[k]) >= charLength:\n",
    "        labels.append(0)\n",
    "        documents.append(vectorizeData(validDocsDict[k]))\n",
    "        charList.extend(vectorizeData(validDocsDict[k]))\n",
    "        concCount += 1\n",
    "        concLengthTotal += len(validDocsDict[k])\n",
    "    elif k.startswith(\"discussion\") and len(validDocsDict[k]) >= charLength:\n",
    "        labels.append(1)\n",
    "        documents.append(vectorizeData(validDocsDict[k]))\n",
    "        charList.extend(vectorizeData(validDocsDict[k]))\n",
    "        discCount += 1\n",
    "        discLengthTotal += len(validDocsDict[k])\n",
    "\n",
    "charList = set(charList)\n",
    "        \n",
    "#for k in validDocsDict2.keys():\n",
    "#    if k.startswith(\"conclusion\"):\n",
    "#        testPubLabels.append(\"conclusion\")\n",
    "#        testPubDocuments.append(vectorizeData(validDocsDict2[k]))\n",
    "#        concCount += 1\n",
    "#        concLengthTotal += len(validDocsDict2[k])\n",
    "#    elif k.startswith(\"discussion\"):\n",
    "#        testPubLabels.append(\"discussion\")\n",
    "#        testPubDocuments.append(vectorizeData(validDocsDict2[k]))\n",
    "#        discCount += 1\n",
    "#        discLengthTotal += len(validDocsDict2[k])\n",
    "        \n",
    "#for k in combinedDicts.keys():\n",
    "#    if k.startswith(\"conclusion\"):\n",
    "#        allDocuments.append(vectorizeData(combinedDicts[k]))\n",
    "#    elif k.startswith(\"discussion\"):\n",
    "#        allDocuments.append(vectorizeData(combinedDicts[k]))\n",
    "        \n",
    "print(len(documents))\n",
    "print(concLengthTotal * 1.0/ concCount)\n",
    "print(discLengthTotal * 1.0/ discCount)\n",
    "\n",
    "\n",
    "train, test, labelsTrain, labelsTest = train_test_split(documents, labels, test_size = 0.95)\n",
    "test1, test2, labelsTest1, labelsTest2 = train_test_split(test, labelsTest, test_size = 0.9)\n",
    "print(len(train))\n",
    "print(len(labelsTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npVecs = np.eye(len(charList))\n",
    "numToVec = dict()\n",
    "labelsToVec = dict()\n",
    "labelsToVec[0] = np.array([1,0])\n",
    "labelsToVec[1] = np.array([0,1])\n",
    "counter = 0\n",
    "for item in charList:\n",
    "    numToVec[item] = npVecs[counter]\n",
    "    counter += 1\n",
    "X_train = np.array([np.array([numToVec[x[y]] for y in x]) for x in train])\n",
    "Y_train = np.array([np.array(labelsToVec[x]) for x in labelsTrain])\n",
    "X_test = np.array([np.array([numToVec[x[y]] for y in x]) for x in test1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 1014, 86)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "#X_train = np.expand_dims(X_train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       ..., \n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VGG-like convolution stack\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(1024, 7, border_mode = 'valid', input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Convolution1D(1024, 7, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Convolution1D(1024, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Convolution1D(1024, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Convolution1D(1024, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Convolution1D(1024, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2025 samples, validate on 225 samples\n",
      "Epoch 1/100\n",
      " 688/2025 [=========>....................] - ETA: 124s - loss: 6.9724 - acc: 0.5654"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=100, batch_size=BATCH_SIZE, verbose=1, \n",
    "          show_accuracy=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_guess = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numCorrect = 0\n",
    "for item in range(len(labelsTest1)):\n",
    "    if Y_guess[item] == labelsTest1[item]:\n",
    "        numCorrect += 1\n",
    "print(numCorrect)\n",
    "print(numCorrect * 1.0 / len(labelsTest1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
