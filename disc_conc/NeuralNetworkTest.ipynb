{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Test\n",
    "\n",
    "This script attempts to create a neural network to solve the problem of classifying a document as part of discussion or conclusion section in scientific papers. This attempts to replicate work done in the paper \"Character-level Convolutional Networks for Text Classification\" by Zhang. \n",
    "\n",
    "Here we input our libraries and set some basic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import theano\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "FIELD_SIZE = 5 * 300\n",
    "STRIDE = 1\n",
    "N_FILTERS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will turn a string into a list of Ascii numbers based on each character in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeData(text):\n",
    "    textList = list(text)\n",
    "    returnList = []\n",
    "    for item in textList[:1014]:\n",
    "        returnList.append(ord(item))\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validDocsDict = dict()\n",
    "fileList = os.listdir(\"BioMedProcessed\")\n",
    "for f in fileList:\n",
    "    validDocsDict.update(pickle.load(open(\"BioMedProcessed/\" + f, \"rb\")))\n",
    "\n",
    "#validDocsDict2 = dict()\n",
    "#fileList = os.listdir(\"PubMedProcessed\")\n",
    "#for f in fileList:\n",
    "#    validDocsDict2.update(pickle.load(open(\"PubMedProcessed/\" + f, \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters for use later. This was developed to handle multiple datasets. Take the conclusion and discussion sections that are at least charLength number of characters. Vectorize that data and put them in the documents list. Then split the data up into multiple different train/test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "documents = []\n",
    "testPubDocuments = []\n",
    "allDocuments = []\n",
    "labels = []\n",
    "testPubLabels = []\n",
    "concLengthTotal = 0\n",
    "discLengthTotal = 0\n",
    "concCount = 0\n",
    "discCount = 0\n",
    "charLength = 1014\n",
    "charList = []\n",
    "\n",
    "#combinedDicts = validDocsDict.copy()\n",
    "#combinedDicts.update(validDocsDict2.copy())\n",
    "\n",
    "for k in validDocsDict.keys():\n",
    "    if k.startswith(\"conclusion\") and len(validDocsDict[k]) >= charLength:\n",
    "        labels.append(0)\n",
    "        documents.append(vectorizeData(validDocsDict[k]))\n",
    "        charList.extend(vectorizeData(validDocsDict[k]))\n",
    "        concCount += 1\n",
    "        concLengthTotal += len(validDocsDict[k])\n",
    "    elif k.startswith(\"discussion\") and len(validDocsDict[k]) >= charLength:\n",
    "        labels.append(1)\n",
    "        documents.append(vectorizeData(validDocsDict[k]))\n",
    "        charList.extend(vectorizeData(validDocsDict[k]))\n",
    "        discCount += 1\n",
    "        discLengthTotal += len(validDocsDict[k])\n",
    "\n",
    "charList = set(charList)\n",
    "        \n",
    "#for k in validDocsDict2.keys():\n",
    "#    if k.startswith(\"conclusion\"):\n",
    "#        testPubLabels.append(\"conclusion\")\n",
    "#        testPubDocuments.append(vectorizeData(validDocsDict2[k]))\n",
    "#        concCount += 1\n",
    "#        concLengthTotal += len(validDocsDict2[k])\n",
    "#    elif k.startswith(\"discussion\"):\n",
    "#        testPubLabels.append(\"discussion\")\n",
    "#        testPubDocuments.append(vectorizeData(validDocsDict2[k]))\n",
    "#        discCount += 1\n",
    "#        discLengthTotal += len(validDocsDict2[k])\n",
    "        \n",
    "#for k in combinedDicts.keys():\n",
    "#    if k.startswith(\"conclusion\"):\n",
    "#        allDocuments.append(vectorizeData(combinedDicts[k]))\n",
    "#    elif k.startswith(\"discussion\"):\n",
    "#        allDocuments.append(vectorizeData(combinedDicts[k]))\n",
    "        \n",
    "print(len(documents))\n",
    "print(concLengthTotal * 1.0/ concCount)\n",
    "print(discLengthTotal * 1.0/ discCount)\n",
    "\n",
    "\n",
    "train, test, labelsTrain, labelsTest = train_test_split(documents, labels, test_size = 0.95)\n",
    "test1, test2, labelsTest1, labelsTest2 = train_test_split(test, labelsTest, test_size = 0.9)\n",
    "print(len(train))\n",
    "print(len(labelsTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an identity matrix from the length of the charList set (to know how many features we have in the set). This identity matrix is used in the one-hot encodding of the characters. For each character in the charList set, we assign a different row of the identity matrix. Then we create X_train and X_test sets using this mapping to convert character ascii numbers to one-hot encoddings. We also create Y_train which maps each section (discussion or conclusion) to a length two one-hot encodded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npVecs = np.eye(len(charList))\n",
    "numToVec = dict()\n",
    "labelsToVec = dict()\n",
    "labelsToVec[0] = np.array([1,0])\n",
    "labelsToVec[1] = np.array([0,1])\n",
    "counter = 0\n",
    "for item in charList:\n",
    "    numToVec[item] = npVecs[counter]\n",
    "    counter += 1\n",
    "X_train = np.array([np.array([numToVec[x[y]] for y in x]) for x in train])\n",
    "Y_train = np.array([np.array(labelsToVec[x]) for x in labelsTrain])\n",
    "X_test = np.array([np.array([numToVec[x[y]] for y in x]) for x in test1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "#X_train = np.expand_dims(X_train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Running the Neural Network\n",
    "\n",
    "Define the model for use in the neural network. This model was taken from the Zhang paper and is attempting to replicate their work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VGG-like convolution stack\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(256, 7, border_mode = 'valid', input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Convolution1D(256, 7, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Convolution1D(256, 3, border_mode = 'valid')) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution1D(256, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Convolution1D(256, 3, border_mode = 'valid')) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution1D(256, 3, border_mode = 'valid')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and start running the model on the X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=5000, batch_size=BATCH_SIZE, verbose=1, \n",
    "          show_accuracy=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted classes for independent test set data, compare them with known labels and output the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_guess = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numCorrect = 0\n",
    "for item in range(len(labelsTest1)):\n",
    "    if Y_guess[item] == labelsTest1[item]:\n",
    "        numCorrect += 1\n",
    "print(numCorrect)\n",
    "print(numCorrect * 1.0 / len(labelsTest1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
