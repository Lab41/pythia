{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Two Datasets Memory Efficient Fuzzy\n",
    "\n",
    "This is a notebook for trying to use topic models for classifying sets of text that are more syntactically similar than topically similar. This notebook attempts to distinguish between discussion and conclusion section of scientific papers. This modifies the sections with random words from the introduction sections. It also reads the second dataset in a more memory efficient way.\n",
    "\n",
    "Below we are loading the two datasets for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "from random import randint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "validDocsDict = dict()\n",
    "fileList = os.listdir(\"BioMedProcessed\")\n",
    "for f in fileList:\n",
    "    validDocsDict.update(pickle.load(open(\"BioMedProcessed/\" + f, \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are setting some vaiables to be used below and defining a function for printing the top words in a topic for the topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(validDocsDict.keys())\n",
    "n_features = 10000\n",
    "n_topics = 2\n",
    "n_top_words = 30\n",
    "lengthOfIntroToAdd = 700\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Here we are preprocessing data for use later. This code only grabs the discussion and conclusion sections of the data. We are also creating appropriate labels for the data and spliting the documents up to train and test sets. We do this for both sets of data and then for a combined set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "53034\n",
      "621.583361617\n",
      "1197.39683976\n",
      "1213\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "documents = []\n",
    "introductionSections = []\n",
    "\n",
    "labels = []\n",
    "concLengthTotal = 0\n",
    "discLengthTotal = 0\n",
    "concCount = 0\n",
    "discCount = 0\n",
    "introCount = 0\n",
    "\n",
    "for k in validDocsDict.keys():\n",
    "    if k.startswith(\"conclusion\"):\n",
    "        labels.append(\"conclusion\")\n",
    "        documents.append(validDocsDict[k])\n",
    "        concCount += 1\n",
    "        concLengthTotal += len(validDocsDict[k].split(' '))\n",
    "    elif k.startswith(\"discussion\"):\n",
    "        labels.append(\"discussion\")\n",
    "        documents.append(validDocsDict[k])\n",
    "        discCount += 1\n",
    "        discLengthTotal += len(validDocsDict[k].split(' '))\n",
    "    elif k.startswith(\"introduction\") and len(validDocsDict[k]) > 10000:\n",
    "        introCount += 1\n",
    "        introductionSections.append(validDocsDict[k])\n",
    "\n",
    "print(len(documents))\n",
    "print(concLengthTotal * 1.0/ concCount)\n",
    "print(discLengthTotal * 1.0/ discCount)\n",
    "print(introCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are reading in the files of the second dataset and only keeping the important sections. We are reading the files in one file at a time to be more memory efficient. Also, note that because the PubMed dataset is much larger, we are only reading in a third of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27688\n",
      "3392\n"
     ]
    }
   ],
   "source": [
    "validDocs2 = []\n",
    "labels2 = []\n",
    "fileList = os.listdir(\"PubMedProcessed\")\n",
    "for f in fileList[0:len(fileList)/3]:\n",
    "    tempDict = pickle.load(open(\"PubMedProcessed/\" + f, \"rb\"))\n",
    "    for item in tempDict.keys():\n",
    "        if item.startswith(\"conclusion\"):\n",
    "            labels2.append(\"conclusion\")\n",
    "            validDocs2.append(tempDict[item])\n",
    "        elif item.startswith(\"discussion\"):\n",
    "            labels2.append(\"discussion\")\n",
    "            validDocs2.append(tempDict[item])\n",
    "        elif item.startswith(\"introduction\") and len(tempDict[item]) > 10000:\n",
    "            introCount += 1\n",
    "            introductionSections.append(tempDict[item])\n",
    "\n",
    "print(len(validDocs2))\n",
    "print(introCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are adding random introduction words to the conclusion and discussion sections to replicate noise. Because the sections are tfidf vectorized, it is not important where in the section they are inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in range(len(documents)):\n",
    "    intro = introductionSections[randint(0, len(introductionSections) - 1)].split(\" \")\n",
    "    randNum = randint(0, len(intro) - lengthOfIntroToAdd)\n",
    "    introWords = intro[randNum:randNum + lengthOfIntroToAdd]\n",
    "    documents[item] = documents[item] + \" \".join(introWords)\n",
    "\n",
    "for item in range(len(validDocs2)):\n",
    "    intro = introductionSections[randint(0, len(introductionSections) - 1)].split(\" \")\n",
    "    randNum = randint(0, len(intro) - lengthOfIntroToAdd)\n",
    "    introWords = intro[randNum:randNum + lengthOfIntroToAdd]\n",
    "    validDocs2[item] = validDocs2[item] + \" \".join(introWords)\n",
    "    \n",
    "train, test, labelsTrain, labelsTest = train_test_split(documents, labels, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting the data up some more to train different models. Discussion and conclusion sections are being put into their own training sets. A TFIDF vectorizer is trained with the whole dataset of conclusion AND discussion sections from both data sets. The multiple different training sets are then transformed using this vectorizer to get vector encodings of the text normalized to sum to 1 which accounts for differing lengths of conclusion and discussion sections and between data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 375.599s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2507: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, norm = 'l1', min_df=2, max_features=n_features, stop_words='english')\n",
    "t0 = time()\n",
    "tf_vectorizer.fit(train)\n",
    "tf = tf_vectorizer.transform(train)\n",
    "\n",
    "tfTest = tf_vectorizer.transform(test)\n",
    "test = tfTest\n",
    "train = tf\n",
    "\n",
    "pubTest = tf_vectorizer.transform(validDocs2)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple two topic LDA on the data and see how that splits the Conclusion and Discussion sections. Then, transform the second dataset using that LDA and see how that splits the Conclusion and Discussion sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=157526 and n_features=10000...\n",
      "done in 617.312s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "study patients health care data risk patient use women research clinical et studies treatment pain al based children information medical population age 1016 participants hiv group authors years disease factors\n",
      "Topic #1:\n",
      "cells cancer cell gene expression genes protein using 10 human figure ml breast used tumor analysis dna al sequence mm et species samples blood proteins mg activity tissue 100 performed\n",
      "Total Conclusion Topic One: 1479\n",
      "Total Conclusion Topic Two: 1196\n",
      "Total Discussion Topic One: 1238\n",
      "Total Discussion Topic Two: 1391\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=100,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "results = lda.transform(test)\n",
    "totalConTop1 = 0\n",
    "totalConTop2 = 0\n",
    "totalDisTop1 = 0\n",
    "totalDisTop2 = 0\n",
    "for x in range(len(results)):\n",
    "    val1 = results[x][0]\n",
    "    val2 = results[x][1]\n",
    "    total = val1 + val2\n",
    "    if val1 > val2:\n",
    "        if labelsTest[x] == \"conclusion\":\n",
    "            totalConTop1 += 1\n",
    "        else:\n",
    "            totalDisTop1 += 1\n",
    "    else:\n",
    "        if labelsTest[x] == \"conclusion\":\n",
    "            totalConTop2 += 1\n",
    "        else:\n",
    "            totalDisTop2 += 1\n",
    "print(\"Total Conclusion Topic One: \" + str(totalConTop1))\n",
    "print(\"Total Conclusion Topic Two: \" + str(totalConTop2))\n",
    "print(\"Total Discussion Topic One: \" + str(totalDisTop1))\n",
    "print(\"Total Discussion Topic Two: \" + str(totalDisTop2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Conclusion Topic One: 6580\n",
      "Total Conclusion Topic Two: 7264\n",
      "Total Discussion Topic One: 6686\n",
      "Total Discussion Topic Two: 7158\n"
     ]
    }
   ],
   "source": [
    "results = lda.transform(pubTest)\n",
    "totalConTop1 = 0\n",
    "totalConTop2 = 0\n",
    "totalDisTop1 = 0\n",
    "totalDisTop2 = 0\n",
    "for x in range(len(results)):\n",
    "    val1 = results[x][0]\n",
    "    val2 = results[x][1]\n",
    "    total = val1 + val2\n",
    "    if val1 > val2:\n",
    "        if labels2[x] == \"conclusion\":\n",
    "            totalConTop1 += 1\n",
    "        else:\n",
    "            totalDisTop1 += 1\n",
    "    else:\n",
    "        if labels2[x] == \"conclusion\":\n",
    "            totalConTop2 += 1\n",
    "        else:\n",
    "            totalDisTop2 += 1\n",
    "print(\"Total Conclusion Topic One: \" + str(totalConTop1))\n",
    "print(\"Total Conclusion Topic Two: \" + str(totalConTop2))\n",
    "print(\"Total Discussion Topic One: \" + str(totalDisTop1))\n",
    "print(\"Total Discussion Topic Two: \" + str(totalDisTop2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classifiers Between Two Datasets\n",
    "\n",
    "Train and test two Bernoulli classifiers (one where dataset 1 is trained and one where dataset 2 is trained) and print out the results of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887099104305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "classifier.fit(train.toarray(), labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(pubTest.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labels2[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934946574481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "classifier.fit(pubTest.toarray(), labels2)\n",
    "\n",
    "classResults = classifier.predict(train.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTrain[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas = classifier.predict_log_proba(train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-62.5096033846\n",
      "-21.0084772036\n"
     ]
    }
   ],
   "source": [
    "TotalRight = 0\n",
    "TotalWrong = 0\n",
    "numRight = 0\n",
    "numWrong = 0\n",
    "RightNumbers = []\n",
    "WrongNumbers = []\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTrain[item]:\n",
    "        TotalRight += probas[item][0] + probas[item][1]\n",
    "        numRight += 1\n",
    "        RightNumbers.append(probas[item][0] + probas[item][1])\n",
    "    else:\n",
    "        TotalWrong += probas[item][0] + probas[item][1]\n",
    "        numWrong += 1\n",
    "        WrongNumbers.append(probas[item][0] + probas[item][1])\n",
    "\n",
    "print(str(TotalRight * 1.0 / numRight))\n",
    "print(str(TotalWrong * 1.0 / numWrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Between Two Datasets\n",
    "Same concept as above, trying with decision trees instead of Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790631320428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(train.toarray(), labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(pubTest.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labels2[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828430756338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(pubTest.toarray(), labels2)\n",
    "\n",
    "classResults = classifier.predict(train.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTrain[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/tree/tree.py:715: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n"
     ]
    }
   ],
   "source": [
    "probas = classifier.predict_log_proba(train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "TotalRight = 0\n",
    "TotalWrong = 0\n",
    "numRight = 0\n",
    "numWrong = 0\n",
    "RightNumbers = []\n",
    "WrongNumbers = []\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTrain[item]:\n",
    "        TotalRight += probas[item][0] + probas[item][1]\n",
    "        numRight += 1\n",
    "        RightNumbers.append(probas[item][0] + probas[item][1])\n",
    "    else:\n",
    "        TotalWrong += probas[item][0] + probas[item][1]\n",
    "        numWrong += 1\n",
    "        WrongNumbers.append(probas[item][0] + probas[item][1])\n",
    "\n",
    "print(str(TotalRight * 1.0 / numRight))\n",
    "print(str(TotalWrong * 1.0 / numWrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
