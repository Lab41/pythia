{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Two Datasets\n",
    "\n",
    "This is a notebook for trying to use topic models for classifying sets of text that are more syntactically similar than topically similar. This notebook attempts to distinguish between discussion and conclusion section of scientific papers.\n",
    "\n",
    "Below we are loading the two datasets for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "validDocsDict = dict()\n",
    "fileList = os.listdir(\"BioMedProcessed\")\n",
    "for f in fileList:\n",
    "    validDocsDict.update(pickle.load(open(\"BioMedProcessed/\" + f, \"rb\")))\n",
    "\n",
    "validDocsDict2 = dict()\n",
    "fileList = os.listdir(\"PubMedProcessed\")\n",
    "for f in fileList:\n",
    "    validDocsDict2.update(pickle.load(open(\"PubMedProcessed/\" + f, \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are setting some vaiables to be used below and defining a function for printing the top words in a topic for the topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = len(validDocsDict.keys())\n",
    "n_features = 10000\n",
    "n_topics = 2\n",
    "n_top_words = 30\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Here we are preprocessing data for use later. This code only grabs the discussion and conclusion sections of the data. We are also creating appropriate labels for the data and spliting the documents up to train and test sets. We do this for both sets of data and then for a combined set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "documents = []\n",
    "testPubDocuments = []\n",
    "allDocuments = []\n",
    "labels = []\n",
    "testPubLabels = []\n",
    "concLengthTotal = 0\n",
    "discLengthTotal = 0\n",
    "concCount = 0\n",
    "discCount = 0\n",
    "\n",
    "combinedDicts = validDocsDict.copy()\n",
    "combinedDicts.update(validDocsDict2.copy())\n",
    "\n",
    "for k in validDocsDict.keys():\n",
    "    if k.startswith(\"conclusion\"):\n",
    "        labels.append(\"conclusion\")\n",
    "        documents.append(validDocsDict[k])\n",
    "        concCount += 1\n",
    "        concLengthTotal += len(validDocsDict[k].split(' '))\n",
    "    elif k.startswith(\"discussion\"):\n",
    "        labels.append(\"discussion\")\n",
    "        documents.append(validDocsDict[k])\n",
    "        discCount += 1\n",
    "        discLengthTotal += len(validDocsDict[k].split(' '))\n",
    "        \n",
    "for k in validDocsDict2.keys():\n",
    "    if k.startswith(\"conclusion\"):\n",
    "        testPubLabels.append(\"conclusion\")\n",
    "        testPubDocuments.append(validDocsDict2[k])\n",
    "        concCount += 1\n",
    "        concLengthTotal += len(validDocsDict2[k].split(' '))\n",
    "    elif k.startswith(\"discussion\"):\n",
    "        testPubLabels.append(\"discussion\")\n",
    "        testPubDocuments.append(validDocsDict2[k])\n",
    "        discCount += 1\n",
    "        discLengthTotal += len(validDocsDict2[k].split(' '))\n",
    "        \n",
    "for k in combinedDicts.keys():\n",
    "    if k.startswith(\"conclusion\"):\n",
    "        allDocuments.append(combinedDicts[k])\n",
    "    elif k.startswith(\"discussion\"):\n",
    "        allDocuments.append(combinedDicts[k])\n",
    "        \n",
    "print(len(documents))\n",
    "print(concLengthTotal * 1.0/ concCount)\n",
    "print(discLengthTotal * 1.0/ discCount)\n",
    "\n",
    "train, test, labelsTrain, labelsTest = train_test_split(documents, labels, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting the data up some more to train different models. Discussion and conclusion sections are being put into their own training sets. A TFIDF vectorizer is trained with the whole dataset of conclusion AND discussion sections from both data sets. The multiple different training sets are then transformed using this vectorizer to get vector encodings of the text normalized to sum to 1 which accounts for differing lengths of conclusion and discussion sections and between data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainSetOne = []\n",
    "trainSetTwo = []\n",
    "\n",
    "for x in range(len(train)):\n",
    "    if labelsTrain[x] == \"conclusion\":\n",
    "        trainSetOne.append(train[x])\n",
    "    else:\n",
    "        trainSetTwo.append(train[x])\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, norm = 'l1', min_df=2, max_features=n_features, stop_words='english')\n",
    "t0 = time()\n",
    "tf_vectorizer.fit(allDocuments)\n",
    "tf = tf_vectorizer.transform(train)\n",
    "\n",
    "tfSetOne = tf_vectorizer.transform(trainSetOne)\n",
    "tfSetTwo = tf_vectorizer.transform(trainSetTwo)\n",
    "tfTest = tf_vectorizer.transform(test)\n",
    "test = tfTest\n",
    "train = tf\n",
    "trainSetOne = tfSetOne\n",
    "trainSetTwo = tfSetTwo\n",
    "\n",
    "pubTest = tf_vectorizer.transform(testPubDocuments)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA With Two Topics\n",
    "\n",
    "Define an LDA topic model on the whole data set with two topics. This is trying to see if the topic model can define the difference between the two groups automatically and prints the top words per topic. This is only performed on the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=100,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the unknown data through the topic model and calculate which topic it is more associated with according to the ratios. Calculate how many of each type (conclusion and discussion) go into each topic (1 or 2). This is done just for the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = lda.transform(test)\n",
    "totalConTop1 = 0\n",
    "totalConTop2 = 0\n",
    "totalDisTop1 = 0\n",
    "totalDisTop2 = 0\n",
    "for x in range(len(results)):\n",
    "    val1 = results[x][0]\n",
    "    val2 = results[x][1]\n",
    "    total = val1 + val2\n",
    "    print(str(labelsTest[x]) + \" \" + str(val1/total) + \" \" + str(val2/total))\n",
    "    if val1 > val2:\n",
    "        if labelsTest[x] == \"conclusion\":\n",
    "            totalConTop1 += 1\n",
    "        else:\n",
    "            totalDisTop1 += 1\n",
    "    else:\n",
    "        if labelsTest[x] == \"conclusion\":\n",
    "            totalConTop2 += 1\n",
    "        else:\n",
    "            totalDisTop2 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the results from the topic transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Total Conclusion Topic One: \" + str(totalConTop1))\n",
    "print(\"Total Conclusion Topic Two: \" + str(totalConTop2))\n",
    "print(\"Total Discussion Topic One: \" + str(totalDisTop1))\n",
    "print(\"Total Discussion Topic Two: \" + str(totalDisTop2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the parameters for the LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classifiers\n",
    "\n",
    "Train three basic classifiers to solve the problem. Try Gaussian, Bernoulli and K Nearest Neighbors classifiers and calculate how accurate they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(train.toarray(), labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(test.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "classifier.fit(train.toarray(), labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(test.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(train, labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(test)\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Topic Models\n",
    "\n",
    "Define two topic models with 20 topics each, one on discussion sections and one on conclusion sections. Then transform both the train and test sets using both topic models to get 40 features for each sample based on the probability distribution for each topic in each LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaSet1 = LatentDirichletAllocation(n_topics=20, max_iter=100,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "ldaSet2 = LatentDirichletAllocation(n_topics=20, max_iter=100,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldaSet1.fit(trainSetOne)\n",
    "print_top_words(ldaSet1, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldaSet2.fit(trainSetTwo)\n",
    "print_top_words(ldaSet2, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results1 = ldaSet1.transform(train)\n",
    "results2 = ldaSet2.transform(train)\n",
    "\n",
    "resultsTest1 = ldaSet1.transform(test)\n",
    "resultsTest2 = ldaSet2.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = np.hstack((results1, results2))\n",
    "resultsTest = np.hstack((resultsTest1, resultsTest2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two classifiers using the transformed train and test sets from the topic models. Print out the accuracy of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(results, labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(resultsTest)\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(results, labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(resultsTest)\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the results of each sample of 40 features so they sum to 1. Then train two more classifiers using the data and print out the accuracy of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(len(results)):\n",
    "    total = 0\n",
    "    for y in range(len(results[x])):\n",
    "        total += results[x][y]\n",
    "    for y in range(len(results[x])):\n",
    "        results[x][y] = results[x][y]/total\n",
    "        \n",
    "for x in range(len(resultsTest)):\n",
    "    total = 0\n",
    "    for y in range(len(resultsTest[x])):\n",
    "        total += resultsTest[x][y]\n",
    "    for y in range(len(resultsTest[x])):\n",
    "        resultsTest[x][y] = resultsTest[x][y]/total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(results, labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(resultsTest)\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(results, labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(resultsTest)\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTest[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classifiers Between Two Datasets\n",
    "\n",
    "Train and test two Bernoulli classifiers (one where dataset 1 is trained and one where dataset 2 is trained) and print out the results of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "classifier.fit(train.toarray(), labelsTrain)\n",
    "\n",
    "classResults = classifier.predict(pubTest.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == testPubLabels[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "classifier.fit(pubTest.toarray(), testPubLabels)\n",
    "\n",
    "classResults = classifier.predict(train.toarray())\n",
    "\n",
    "numRight = 0\n",
    "\n",
    "for item in range(len(classResults)):\n",
    "    if classResults[item] == labelsTrain[item]:\n",
    "        numRight += 1\n",
    "\n",
    "print(str(numRight * 1.0 / len(classResults) * 1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
