{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Processing for Topic Model Test\n",
    "\n",
    "Getting the data from the repository...don't run unless you don't have the data!\n",
    "\n",
    "!apt-get -y install curl\n",
    "\n",
    "!curl -o BioMedSent/BioMedSentences.tar.zip http://i.stanford.edu/hazy/opendata/bmc/bmc_full_dddb_20150927_9651bf4a468cefcea30911050c2ca6db.tar.bzip2\n",
    "\n",
    "http://i.stanford.edu/hazy/opendata/pmc/pmc_dddb_full_20150927_3b20db570e2cb90ab81c5c6f63babc91.tar.bzip2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "This section defines the Sentence object used when importing and saving the data. Grab the files in a directory and process a subset of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1837864"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Statements\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Sentence object definition for data import and processing \n",
    "class Sentence:\n",
    "    def __init__(self, document, sentenceNumber, wordList, lemmaList, posList):\n",
    "        self.document = document\n",
    "        self.sentenceNumber = sentenceNumber\n",
    "        self.wordList = wordList\n",
    "        self.lemmaList = lemmaList\n",
    "        self.posList = posList\n",
    "        self.sentence = \" \".join([word for word in wordList if word not in string.punctuation])\n",
    "        self.lemmaSent = \" \".join([word for word in lemmaList if word not in string.punctuation])\n",
    "\n",
    "#Get the files we want to process and put them into a list of lists called sentList\n",
    "fileList = os.listdir(\"../PubMed/pmc_dddb_full\")\n",
    "sentList = []\n",
    "fileList.sort()\n",
    "for n in range(27, 28):\n",
    "    f = open(\"../PubMed/pmc_dddb_full/\" + fileList[n], 'r')\n",
    "    for line in f:\n",
    "        sentList.append(line.split('\\t'))\n",
    "\n",
    "len(sentList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have all of the sentences in a list of lists grab the first element of each sentence list (the document id) and add that to a docList. Make this docList a set so we have the number of unique documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9981"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docList = []\n",
    "for thing in sentList:\n",
    "    docList.append(thing[0])\n",
    "\n",
    "len(set(docList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "Define the processSent function for use by the multiprocessing part of the code. This function takes off some of the structure of parts of the data (removing the {,}, and \") and defines the Sentence object with all the appropriate parts.\n",
    "\n",
    "We then use 14 cores (if available) for the Pool object and apply the processSent function to every sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentObjList = []\n",
    "def processSent(item):\n",
    "    wordList = item[3].replace('\"',\"\").lstrip(\"{\").rstrip(\"}\").split(\",\")\n",
    "    wordList = filter(None, wordList)\n",
    "    posList = item[4].split(\",\")\n",
    "    lemmaList = item[6].replace('\"',\"\").lstrip(\"{\").rstrip(\"}\").split(\",\")\n",
    "    lemmaList = filter(None, lemmaList)\n",
    "    return Sentence(item[0], item[1], wordList, lemmaList, posList)\n",
    "\n",
    "po = Pool(16)\n",
    "results = [po.apply_async(processSent, args = (sent,)) for sent in sentList]\n",
    "po.close()\n",
    "po.join()\n",
    "output = [p.get() for p in results]\n",
    "sentObjList = output\n",
    "sentObjList[7].lemmaSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that the sentences are processed, we need to find which sections these sentences should be atributed. For most of these papers, section headers are one word sentences. We are looking for common section headers and saving the sentence numbers for that section in that document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headingsDict = defaultdict(dict)\n",
    "\n",
    "for sent in sentObjList:\n",
    "    if len(sent.wordList) == 1:\n",
    "        #print(sent.wordList)\n",
    "        word = string.upper(sent.wordList[0]).strip()\n",
    "        if word == 'INTRODUCTION' or word == 'BACKGROUND':\n",
    "            headingsDict[sent.document][\"introduction\"] = sent.sentenceNumber\n",
    "        elif word == 'METHODS':\n",
    "            headingsDict[sent.document][\"methods\"] = sent.sentenceNumber\n",
    "        elif word == 'RESULTS':\n",
    "            headingsDict[sent.document][\"results\"] = sent.sentenceNumber\n",
    "        elif word == 'DISCUSSION':\n",
    "            headingsDict[sent.document][\"discussion\"] = sent.sentenceNumber\n",
    "        elif word == 'CONCLUSION':\n",
    "            headingsDict[sent.document][\"conclusion\"] = sent.sentenceNumber\n",
    "        elif word == 'REFERENCES':\n",
    "            headingsDict[sent.document][\"references\"] = sent.sentenceNumber\n",
    "        \n",
    "\n",
    "headingsDict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now the sentences need to be tagged to their appropriate section and concatenated into one string per section per document.\n",
    "\n",
    "The sentences are assigned a section by whichever section they are closest to (that is less than their sentence number). For example, if introduction had sentence number 5 and methods had sentence number 25, sentence number 20 would be assigned to introduction.\n",
    "\n",
    "This is done for each sentence in each document and joined by spaces into a one string per section per document. Finally, only the documents that contain an introduction, discussion, and conclusion are kept and put into the validDocsDict dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documentDict = defaultdict(list)\n",
    "docPartsDict = defaultdict(lambda : defaultdict(list))\n",
    "docPartsCombinedDict = defaultdict(dict)\n",
    "\n",
    "for item in sentObjList:\n",
    "    documentDict[item.document].append(item)\n",
    "    \n",
    "for document in documentDict.keys():\n",
    "    docSentList = documentDict[document]\n",
    "    introNum = int(headingsDict[document].get(\"introduction\", -1))\n",
    "    methoNum = int(headingsDict[document].get(\"methods\", -1))\n",
    "    resultNum = int(headingsDict[document].get(\"results\", -1))\n",
    "    discussNum = int(headingsDict[document].get(\"discussion\", -1))\n",
    "    conclusionNum = int(headingsDict[document].get(\"conclusion\", -1))\n",
    "    refNum = int(headingsDict[document].get(\"references\", -1))\n",
    "\n",
    "    for sent in docSentList:\n",
    "        label = \"noSection\"\n",
    "        dist = int(sent.sentenceNumber)\n",
    "        sentNumber = int(sent.sentenceNumber)\n",
    "        \n",
    "        if dist > sentNumber - introNum and sentNumber - introNum > 0:\n",
    "            label = \"introduction\"\n",
    "            dist = sentNumber - introNum\n",
    "        if dist > sentNumber - methoNum and sentNumber - methoNum > 0:\n",
    "            label = \"methods\"\n",
    "            dist = sentNumber - methoNum\n",
    "        if dist > sentNumber - resultNum and sentNumber - resultNum > 0:\n",
    "            label = \"results\"\n",
    "            dist = sentNumber - resultNum\n",
    "        if dist > sentNumber - discussNum and sentNumber - discussNum > 0:\n",
    "            label = \"discussion\"\n",
    "            dist = sentNumber - discussNum\n",
    "        if dist > sentNumber - conclusionNum and sentNumber - conclusionNum > 0:\n",
    "            label = \"conclusion\"\n",
    "            dist = sentNumber - conclusionNum\n",
    "        if dist > sentNumber - refNum and sentNumber - refNum > 0:\n",
    "            label = \"references\"\n",
    "            dist = sentNumber - refNum\n",
    "        if sent.sentence.strip().lower() not in [\"introduction\", \"methods\", \"results\", \"discussion\", \"conclusion\", \"references\"]:\n",
    "            docPartsDict[document][label].append(sent)\n",
    "    \n",
    "    for x in docPartsDict[document].keys():\n",
    "        docPartsCombinedDict[document][x] = \" \".join(y.sentence for y in sorted(docPartsDict[document][x], key=lambda z: z.sentenceNumber))\n",
    "\n",
    "validDocsDict = defaultdict(dict)\n",
    "\n",
    "for doc in docPartsCombinedDict.keys():\n",
    "    tempKeys = docPartsCombinedDict[doc].keys()\n",
    "    if 'introduction' in tempKeys and 'discussion' in tempKeys and 'conclusion' in tempKeys:\n",
    "        validDocsDict[doc] = docPartsCombinedDict[doc]\n",
    "\n",
    "print(str(len(docPartsCombinedDict.keys())))\n",
    "print(str(len(validDocsDict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Take the valid documents in the validDocsDict and output to a pickle file with the key part_docid with the part being introduction, methods, etc. and the docid allowing for document tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDict = dict()\n",
    "for doc in validDocsDict.keys():\n",
    "    for part in validDocsDict[doc].keys():\n",
    "        outputDict[part + \"_\" + doc] = validDocsDict[doc][part]\n",
    "\n",
    "pickle.dump(outputDict, open(\"TestDocsPub27.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
