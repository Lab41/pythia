{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert MongoDB Output to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a mongo database configured for pythia experiments and creates pandas dataframes in order to more easily compare experiments.  The first data frame contains all of the variables from the config and results fields in the database.  The second dataframe, if SAVE_RESULTS is flagged in the experiements, creates fields for each observation.  Error rate for each observation is also calculated.  The ability to export each dataframe as csv is also included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pymongo import MongoClient\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code connects to MongoDB by allowing you to enter your host, port, and database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host = input() #host as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "port = int(input()) #port as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to Mongo host & port\n",
    "\n",
    "client = MongoClient(host, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input the name of the database you'd like to connect to. Example 'sacred_demo' or 'pythia_experiment'\n",
    "\n",
    "db_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "\n",
    "db = client.get_database(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell creates a dataframe from the database by combining the 'config' and 'result' fields.  It takes the 2-values fields recall, precision, f score, and support and creates new columns VARIABLE_dup and VARIABLE_nov for each.  This also allows CSV output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cursor = db.default.runs.find()\n",
    "#count = 0\n",
    "list_of_dics = []\n",
    "for doc in cursor:\n",
    "    if doc['status'] == 'COMPLETED':\n",
    "        #Create algorithm column with choice of algorithm\n",
    "        if doc['config']['LOG_REG'] == True:\n",
    "            doc['config']['algorithms'] = 'LOG_REG'\n",
    "        elif doc['config']['SVM'] == True:\n",
    "            doc['config']['algorithms'] = 'SVM'\n",
    "        else: \n",
    "            doc['config']['algorithms'] = 'XGB'\n",
    "\n",
    "        feature_list = []\n",
    "        W2V_features = []\n",
    "        #Add features to feature list to create identifier\n",
    "        for feature in doc['config'].keys():\n",
    "            if feature.startswith(\"MEM\"):\n",
    "                 if doc['config'][\"MEM_NET\"] == True:\n",
    "                    feature_list.append(feature)\n",
    "\n",
    "            elif doc['config'][feature] == True and type(doc['config'][feature]) == bool :\n",
    "                feature_list.append(feature)\n",
    "            elif feature.startswith(\"W2V\"):\n",
    "                W2V_features.append(feature + \":\" + str(doc['config'][feature]))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # add LDA and WORDONEHOT features to featurelist if they are active in model\n",
    "        feature_list2 = []\n",
    "        for item in feature_list:\n",
    "            if item.startswith(\"LDA\"):\n",
    "                feature_list2.append(\"LDA_TOPICS:\" + str(doc['config']['LDA_TOPICS']))\n",
    "            elif item.startswith(\"WORDONEHOT\"):\n",
    "                feature_list2.append(\"WORDONEHOT_VOCAB:\" + str(doc['config']['WORDONEHOT_VOCAB']))\n",
    "        full_feature_list = feature_list + feature_list2\n",
    "        doc['config'][\"index\"] = \"_\".join(str(e) for e in feature_list)\n",
    "\n",
    "        #Separate out recall into recall_dup and recall_nov\n",
    "        combined_dics = doc['config'].copy()     \n",
    "        doc[\"result\"][\"result_dup\"] = doc['result'][\"recall\"][0]\n",
    "        doc[\"result\"][\"recall_nov\"] = doc['result'][\"recall\"][1]\n",
    "\n",
    "        doc[\"result\"][\"precision_dup\"] = doc['result'][\"precision\"][0]\n",
    "        doc[\"result\"][\"precision_nov\"] = doc['result'][\"precision\"][1]\n",
    "\n",
    "        doc[\"result\"][\"fscore_dup\"] = doc['result'][\"f score\"][0]\n",
    "        doc[\"result\"][\"fscore_nov\"] = doc['result'][\"f score\"][1]\n",
    "\n",
    "        doc[\"result\"][\"support_dup\"] = doc['result'][\"support\"][0]\n",
    "        doc[\"result\"][\"support_nov\"] = doc['result'][\"support\"][1]\n",
    "\n",
    "        #remove original recall,precision, fscore, and support\n",
    "        doc[\"result\"].pop(\"recall\")\n",
    "        doc[\"result\"].pop(\"precision\")\n",
    "        doc[\"result\"].pop(\"f score\")\n",
    "        doc[\"result\"].pop(\"support\")\n",
    "\n",
    "\n",
    "        #combine dictionaries\n",
    "        combined_dics.update(doc[\"result\"])\n",
    "\n",
    "        #add to list of dics\n",
    "        list_of_dics.append(combined_dics)\n",
    "allvalues_df = pd.DataFrame(list_of_dics)\n",
    "allvalues_df = allvalues_df.set_index(\"index\")\n",
    "display(allvalues_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, save dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_output = input() #name for csv output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allvalues_df.to_csv(csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates a dataframe that has an observation for each column and an experiment for each row.  Also included is a row for ground truth and error rate for each observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This line will delete all rows where the predicted label is null\n",
    "allvalues_df = allvalues_df[pd.notnull(allvalues_df['predicted_label'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create Data frame with observations as columns and models as rows\n",
    "label_df = pd.DataFrame(allvalues_df['predicted_label'].tolist(), columns=allvalues_df['id'][0])\n",
    "#Add name as row index\n",
    "label_df['index'] = allvalues_df.index.values\n",
    "label_df = label_df.set_index('index')\n",
    "label_df = label_df.drop('index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make dataframe for Ground Truth Values\n",
    "novelty_df = pd.DataFrame(allvalues_df['novelty'][0]).transpose()\n",
    "novelty_df.columns = list(label_df)\n",
    "novelty_df['index'] = ['Ground_Truth']\n",
    "novelty_df = novelty_df.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create combined dataframe with ground truth and labeled predictions for each experiment\n",
    "labeled_prediction_df = label_df.append(novelty_df)\n",
    "labeled_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the error rate is computed for each observation and added as a row in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_rate = abs(label_df.sum()/len(label_df) - novelty_df.sum())\n",
    "error_rate = error_rate.rename(\"error_rate\")\n",
    "labeled_prediction_df = labeled_prediction_df.append(error_rate)\n",
    "labeled_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code reshapes the data to be \"tall\" where the experiments are the coluns and the observations are rows.  In addition, the ground truth and error rate are columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make data tall\n",
    "tall_df = labeled_prediction_df.transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print tall df\n",
    "tall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot a histogram of error rate for each observation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.figure();\n",
    "tall_df[\"error_rate\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the tall dataframe can be exported to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tall_csv_output = input() #name for csv output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tall_df.to_csv(tall_csv_output) #call to create csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, convert wide dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wide_csv_output = input() #name for csv output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_prediction_df.to_csv(wide_csv_output) #call to create csv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3-pythia]",
   "language": "python",
   "name": "conda-env-py3-pythia-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
