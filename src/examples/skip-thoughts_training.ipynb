{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coding data paths\n",
    "\n",
    "Mostly this notebook should just run. It however requires the user to deal with one of the next 2 cells.\n",
    "\n",
    "#### Location of the data\n",
    "\n",
    "`sample_location` should be the path to a directory which contains your data. Each file should contain json-parsable lines. The directory can have subdirectories. The code will recursively find the files. There should be no `.json` files anywhere in the directory except those the code wishes to parse.\n",
    "\n",
    "`path_to_word2vec` is a `.bin` word2vec file the code depends on, e.g. the Google News model founds at https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "#### Where to put output\n",
    "\n",
    "`parsed_data_location` is a directory of `.csv` files the code will create structured the same as `sample_location`, but where the sentences have been normalized an tokenized, and where each file reprents a post.\n",
    "\n",
    "`training_data_location` is the name of a file that will store the sentences in a single file, one per line, with null characters separating blog posts.\n",
    "\n",
    "`vocab_location` should be the name of a pickle file (including path), which will store information about the words in the corpus\n",
    "\n",
    "`model_location` should be the name of a .npz (zipped numpy) file (including path), which will store the model itself as a numpy array. The code will also create a .npz.pkl file with the same name containing some metadata.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_location = '/Users/chrisn/mad-science/pythia/data/stackexchange'\n",
    "path_to_word2vec = '/Users/chrisn/mad-science/pythia/data/stackexchange/model/word2vecAnime.bin'\n",
    "parsed_data_location = '/Users/chrisn/testing'\n",
    "training_data_location = '/Users/chrisn/testing/training.txt'\n",
    "vocab_location = '/Users/chrisn/mad-science/pythia/data/stackexchange/model/vocab.pickle'\n",
    "model_location = '/Users/chrisn/mad-science/pythia/data/stackexchange/model/corpus.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much easier to take the contents of the above cell, stick it in a file in your python path called my_config.py, and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_config import *\n",
    "# The code will look for all files in sample_location which end with file_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I agree there are better ways to do that, either taking advantage of environment variables of parsing the config file using the `configparser` module.\n",
    "\n",
    "I only had 2 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's import some modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import auxillary modules\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import csv\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import theano\n",
    "import theano\n",
    "import theano.tensor as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May need to set the flag if your .theanorc isn't correct. If you want to run on gpu, you should fix your .theanorc\n",
    "# And make this cell irrelevant\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Double check that floatX is float 32\n",
    "# device should be either cpu or gpu, as desired.\n",
    "print(theano.config.floatX)\n",
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this next cell is my bad. The notebook only runs if your paths are all configured right. You may need to adjust the below cell to import pythia/skipthoughts models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-995dc513c656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import skipthoughts modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/chrisn/mad-science/pythia/src/featurizers/skipthoughts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import pythia modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrisn/mad-science/pythia/src/featurizers/skipthoughts/training/vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mConstructing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloading\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "# Import skipthoughts modules\n",
    "sys.path.append('/Users/chrisn/mad-science/pythia/src/featurizers/skipthoughts')\n",
    "from training import vocab, train, tools\n",
    "import skipthoughts\n",
    "# Import pythia modules\n",
    "sys.path.append('/Users/chrisn/mad-science/pythia/')\n",
    "from src.utils import normalize, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For evaluation purposes, import some sklearn modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Because there were a lot of annoying warnings.\n",
    "# The Beautiful Soup module as used in the pythia normalization is mad about something\n",
    "# And the skip-thoughts code is full of deprecation warnings about how numpy works. The warnings can crash my system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and normalization\n",
    "\n",
    "Who knows the best way to do this? I tried to match the expectations of both the skip-thoughts code and the pythia codebase as best I could.\n",
    "\n",
    "For each document:\n",
    "\n",
    "1) Make list of sentences. We use utils.tokenize.punkt_sentences\n",
    "\n",
    "2) Normalize each sentence. Remove html and make everything lower-case. We use utils.normalize.xml_normalize\n",
    "\n",
    "3) Tokenize each sentence. Now each sentence is a string of space-separated tokens. We use utils.tokenize.word_punct_tokens and rejoin the tokens.\n",
    "\n",
    "Because I had so many difficulties with things crashing, I was happy whenever I got anything done and wanted to save where I was. I also became gunshy about using memory. The solution below is thus entirely streaming. This slows it down because of file i/o.\n",
    "\n",
    "The output of this section run on the entire stackexchange corpus can be found in /data/fs4/datasets/stackexchange_models/se_posts_parsed.tar.gz.\n",
    "\n",
    "(Well, the tarring was done in the shell. This cell just creates a directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_extension = \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instead of trying to parse in memory, can instead parse line by line and write to disk\n",
    "fieldnames = [\"body_text\", \"post_id\",\"cluster_id\", \"order\", \"novelty\"]\n",
    "for root,dirs,files in os.walk(sample_location):\n",
    "    for doc in files:\n",
    "        if doc.endswith(file_extension):\n",
    "            for line in open(os.path.join(sample_location,root,doc)):\n",
    "                temp_dict = json.loads(line)\n",
    "                post_id = temp_dict['post_id']\n",
    "                text = temp_dict['body_text']\n",
    "                sentences = tokenize.punkt_sentences(text)\n",
    "                normal = [normalize.xml_normalize(sentence) for sentence in sentences]\n",
    "                tokens = [' '.join(tokenize.word_punct_tokens(sentence)) for sentence in normal]\n",
    "                base_doc = doc.split('.')[0]\n",
    "                output_filename = \"{}_{}.csv\".format(base_doc,post_id)\n",
    "                rel_path = os.path.relpath(root,sample_location)\n",
    "                output_path = os.path.join(parsed_data_location,rel_path,output_filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok = True)\n",
    "                with open(output_path,'w') as token_file:\n",
    "                    #print(parsed_data_location,rel_path,output_filename)\n",
    "                    writer = csv.DictWriter(token_file,fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    output_dict = temp_dict\n",
    "                    for token in tokens:\n",
    "                        output_dict['body_text'] = token\n",
    "                        writer.writerow(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.relpath(sample_location,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.split(sample_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An annoying issue\n",
    "\n",
    "`tokenized` is now a list of lists. Each inner list represents a document as a list of strings, where each string represents a sentence.\n",
    "\n",
    "The trainer expects a list of sentences. To match expectations, those inner brackets need to disappear.\n",
    "\n",
    "However, this then looks like we have one real long document where the documents have been smashed together in arbitrary order. And the training will mistake the first sentence of one document as being part of the context of the last sentence of another. For sufficiently long documents, you can argue this is just noise. For documents that are themselves only a few sentences, this seems like too much noise.\n",
    "\n",
    "My cludgy fix is to introduce a sentence consisting of a single null character `'\\0'` and add this sentence between every document when concatenating. This may have unintended side-effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_separator = '\\0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "cluster_ids = []\n",
    "with open(training_data_location,'w') as outfile:\n",
    "    for root, dirs, files in os.walk(parsed_data_location):\n",
    "        for doc in files:\n",
    "            if doc.endswith('.csv'):\n",
    "                for line in csv.DictReader(open(os.path.join(root,doc))):\n",
    "                    outfile.write(line['body_text'] + '\\n')\n",
    "                    sentences.append(line['body_text'])\n",
    "                    cluster_ids.append(int(line['cluster_id']))\n",
    "                outfile.write(doc_separator + '\\n')\n",
    "                cluster_ids.append(-1)\n",
    "cluster_ids = numpy.array(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can skip this cell if sentences is still in memory\n",
    "sentences = [x.strip() for x in open(training_data_location).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordcount the count of words, ordered by appearance in text\n",
    "# worddict \n",
    "worddict, wordcount = vocab.build_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.save_dictionary(worddict, wordcount, vocab_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters\n",
    "\n",
    "Definitely set:\n",
    "* saveto: a path where the model will be periodically saved\n",
    "* dictionary: where the dictionary is.\n",
    "\n",
    "Consider tuning:\n",
    "* dim_word: the dimensionality of the RNN word embeddings (Default 620)\n",
    "* dim: the size of the hidden state (Default 2400)\n",
    "* max_epochs: the total number of training epochs (Default 5)\n",
    "\n",
    "* decay_c: weight decay hyperparameter (Default 0, i.e. ignored)\n",
    "* grad_clip: gradient clipping hyperparamter (Default 5)\n",
    "* n_words: the size of the decoder vocabulary (Default 20000)\n",
    "* maxlen_w: the max number of words per sentence. Sentences longer than this will be ignored (Default 30)\n",
    "* batch_size: size of each training minibatch (roughly) (Default 64)\n",
    "* saveFreq: save the model after this many weight updates (Default 1000)\n",
    "\n",
    "Other options:\n",
    "* displayFreq: display progress after this many weight updates (Default 1)\n",
    "* reload_: whether to reload a previously saved model (Default False)\n",
    "\n",
    "## Some obvervations on parameters\n",
    "\n",
    "The default displayFreq is 1. Which seems low. It means every iteration prints something. It seems excessive. I suggest 100.\n",
    "\n",
    "As long as the computer can handle it in memory, a bigger batch size seems better all around. I am trying 256.\n",
    "\n",
    "A good chunk of stackexchange sentences seemed to be at least 30 tokens. I am changing that setting to 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using a small set of paramters for testing\n",
    "params = dict(\n",
    "    saveto = model_location,\n",
    "    dictionary = vocab_location,\n",
    "    n_words = 1000,\n",
    "    dim_word = 100,\n",
    "    dim = 500,\n",
    "    max_epochs = 1,\n",
    "    saveFreq = 100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.trainer(sentences,**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model created doesn't quite fit into the pipeline, because it is a \"uni-skip\" model, not a \"combine skip\" model. The pipeline uses skipthoughts.encode, which requires very particularly formatted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tools.load_model([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Having a lot of trouble getting this line to not crash.\n",
    "tools.encode(model,sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate?\n",
    "\n",
    "Supervised task. Apply cluster_id as label to each sentence. Run regression. Evaluate performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a percentage of the data to be the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_percent = 1 #Choose a subsample of the data\n",
    "holdout_percent = 0.5 #Of that subsample, make this amount training data and the rest testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the sentences\n",
    "sentences = [x.strip() for x in open(training_data_location).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in cluster ids\n",
    "cluster_ids = []\n",
    "for root, dirs, files in os.walk(parsed_data_location):\n",
    "    for doc in files:\n",
    "        if doc.endswith('.csv'):\n",
    "            for line in csv.DictReader(open(os.path.join(root,doc))):\n",
    "                cluster_ids.append(int(line['cluster_id']))\n",
    "            cluster_ids.append(-1)\n",
    "cluster_ids = numpy.array(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check. Should be true.\n",
    "num_sentences == len(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = numpy.arange(num_sentences)\n",
    "num_samples = int(evaluation_percent * num_sentences)\n",
    "index_sample = numpy.sort(numpy.random.choice(indices, size=num_samples, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_sentences = [sentences[i] for i in index_sample]\n",
    "sample_clusters = cluster_ids[index_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell can easily kill my notebook's memory\n",
    "# Instead I recommend the command-line scri\n",
    "model = tools.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Broken!!!\n",
    "#encodings = tools.encode(model, sample_sentences)\n",
    "encodings = numpy.random.rand(num_samples,10) #Since I can't get encodings to actually work. Here are some numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding_train, encoding_test, cluster_train, cluster_test = train_test_split(encodings,\n",
    "                                                                              sample_clusters,\n",
    "                                                                              test_size = holdout_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(encoding_train, cluster_train)\n",
    "regression.predict(encoding_test)\n",
    "regression.score(encoding_test, cluster_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An in-memory approach.\n",
    "\n",
    "Because everything kept crashing on me, I ultimately found it most convenient to do everything in a streaming fashion with a lot of writing to disk at every stage. This is obviously slower than desirable. Basically I do a thing, write out the results, read the results back in, then do the next thing.\n",
    "\n",
    "Below is an in-memory approach that reads everything into memory and pushes forward, still sometimes saving key steps to disk, but without any rereading in. Because of various technical issues, this code has never been tested at scale. It works on the anime dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell may be too memory inefficient\n",
    "doc_dicts = [json.loads(line)\n",
    "            for root,dirs,files in os.walk(stackoverflow_sample_location)\n",
    "            for doc in files\n",
    "            for line in open(os.path.join(stackoverflow_sample_location,root,doc))\n",
    "            ]\n",
    "# doc_dicts is a list of dictionaries, each containing document data\n",
    "# In the anime sample, the text is labeled 'body_text'\n",
    "# There is a field cluster_id which we will use as the categorical label\n",
    "cluster_ids = [d['cluster_id'] for d in doc_dicts]\n",
    "docs = [d['body_text'] for d in doc_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(doc_dicts) # For efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of sentences for each doc\n",
    "sentenced = [tokenize.punkt_sentences(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize each sentence\n",
    "normalized = [[normalize.xml_normalize(sentence) for sentence in doc] for doc in sentenced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(sentenced) #If you're done with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize each sentence\n",
    "tokenized = [[' '.join(tokenize.word_punct_tokens(sentence)) for sentence in doc] for doc in normalized]\n",
    "\n",
    "json.dump(tokenized,open('/Users/chrisn/mad-science/pythia/data/book_corpus/model/tokenized.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separated = sum(zip(tokenized,[[doc_separator]]*len(tokenized)),tuple())\n",
    "sentences = sum(separated,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separated = sum(zip(tokenized,[[doc_separator]]*len(tokenized)),tuple())\n",
    "sentences = sum(separated,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "6164553/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Word2Vec.load_word2vec_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_map = Word2Vec.load_word2vec_format(path_to_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_map.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = tools.load_googlenews_vectors('/Users/chrisn/mad-science/pythia/data/google_news/GoogleNews-vectors-negative300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
