{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coding data paths\n",
    "\n",
    "Mostly this notebook should just run. It however requires the user to fill in the cell below.\n",
    "\n",
    "vocab_location should be the name of a pickle file (including path), which will store information about the words in the corpus\n",
    "\n",
    "model_location should be the name of a .npz (zipped numpy) file (including path), which will store the model itself as a numpy array.\n",
    "\n",
    "sample_location should be the path to a directory which contains your data. Each file should contain json-parsable lines. The directory can have subdirectories. The code will recursively find the files. There should be no files anywhere in the directory except those the code wishes to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_config import *\n",
    "# The code will look for all files in sample_location which end with file_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import auxillary modules\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import skipthoughts modules\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "sys.path.append('/Users/chrisn/mad-science/pythia/src/featurizers/')\n",
    "from training import vocab, train, tools\n",
    "import skipthoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import hyperopt modules\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import pythia modules\n",
    "sys.path.append('/Users/chrisn/mad-science/pythia/')\n",
    "from src.utils import normalize, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For evaluation purposes, import some sklearn modules\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Because there were a lot of annoying warnings. Ignore this cell if you want to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell may be too memory inefficient\n",
    "doc_dicts = [json.loads(line)\n",
    "            for root,dirs,files in os.walk(stackoverflow_sample_location)\n",
    "            for doc in files\n",
    "            for line in open(os.path.join(stackoverflow_sample_location,root,doc))\n",
    "            ]\n",
    "# doc_dicts is a list of dictionaries, each containing document data\n",
    "# In the anime sample, the text is labeled 'body_text'\n",
    "# There is a field cluster_id which we will use as the categorical label\n",
    "cluster_ids = [d['cluster_id'] for d in doc_dicts]\n",
    "docs = [d['body_text'] for d in doc_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and normalization\n",
    "\n",
    "Who knows the best way to do this? I tried to match the expectations of both the skip-thoughts code and the pythia codebase as best I could.\n",
    "\n",
    "For each document:\n",
    "\n",
    "1) Make list of sentences. We use utils.tokenize.punkt_sentences\n",
    "\n",
    "2) Normalize each sentence. Remove html and make everything lower-case. We use utils.normalize.xml_normalize\n",
    "\n",
    "3) Tokenize each sentence. Now each sentence is a string of space-separated tokens. We use utils.tokenize.word_punct_tokens and rejoin the tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of sentences for each doc\n",
    "sentenced = [tokenize.punkt_sentences(doc) for doc in docs]\n",
    "# Normalize each sentence\n",
    "normalized = [[normalize.xml_normalize(sentence) for sentence in doc] for doc in sentenced]\n",
    "#Tokenize each sentence\n",
    "tokenized = [[' '.join(tokenize.word_punct_tokens(sentence)) for sentence in doc] for doc in normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(tokenized,open('/Users/chrisn/mad-science/pythia/data/book_corpus/model/tokenized.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instead of trying to parse in memory, can instead parse line by line and write to disk\n",
    "fieldnames = [\"body_text\", \"post_id\",\"cluster_id\", \"order\", \"novelty\"]\n",
    "for root,dirs,files in os.walk(sample_location):\n",
    "    for doc in files:\n",
    "        if doc.endswith(file_extension):\n",
    "            for line in open(os.path.join(sample_location,root,doc)):\n",
    "                temp_dict = json.loads(line)\n",
    "                post_id = temp_dict['post_id']\n",
    "                text = temp_dict['body_text']\n",
    "                sentences = tokenize.punkt_sentences(text)\n",
    "                normal = [normalize.xml_normalize(sentence) for sentence in sentences]\n",
    "                tokens = [' '.join(tokenize.word_punct_tokens(sentence)) for sentence in normal]\n",
    "                base_doc = doc.split('.')[0]\n",
    "                output_filename = \"{}_{}.csv\".format(base_doc,post_id)\n",
    "                rel_path = os.path.relpath(root,sample_location)\n",
    "                output_path = os.path.join(parsed_data_location,rel_path,output_filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok = True)\n",
    "                with open(output_path,'w') as token_file:\n",
    "                    #print(parsed_data_location,rel_path,output_filename)\n",
    "                    writer = csv.DictWriter(token_file,fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    output_dict = temp_dict\n",
    "                    for token in tokens:\n",
    "                        output_dict['body_text'] = token\n",
    "                        writer.writerow(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.relpath(sample_location,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.split(sample_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category labels\n",
    "\n",
    "Each document had a cluster_id. We use this cluster_id as the categorical label for each sentence.\n",
    "\n",
    "We create a numpy array of shape num_sentences by (num_clusters + 1). The extra cluster is for the null sentences we will mention shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section currently incomplete. Please disregard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = []\n",
    "for doc_index, cluster_id in enumerate(cluster_ids):\n",
    "    num_sentences = len(tokenized[doc_index]) # The number of sentences in the current document\n",
    "    for i in range(num_sentences + 1:\n",
    "                  target.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob.glob(sample_location,\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob.glob(sample_location+\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_location+\"*.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An annoying issue\n",
    "\n",
    "`tokenized` is now a list of lists. Each inner list represents a document as a list of strings, where each string represents a sentence.\n",
    "\n",
    "The trainer expects a list of sentences. To match expectations, those inner brackets need to disappear.\n",
    "\n",
    "However, this then looks like we have one real long document where the documents have been smashed together in arbitrary order. And the training will mistake the first sentence of one document as being part of the context of the last sentence of another. For sufficiently long documents, you can argue this is just noise. For documents that are themselves only a few sentences, this seems like too much noise.\n",
    "\n",
    "My cludgy fix is to introduce a sentence consisting of a single null character `'\\0'` and add this sentence between every document when concatenating. This may have unintended side-effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_separator = '\\0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(doc_separator+\"\\n\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If tokenized has been written to a filesystem and needs to be read in\n",
    "sentences = []\n",
    "cluster_ids = []\n",
    "with open(training_data_location,'w') as outfile:\n",
    "    for root, dirs, files in os.walk(parsed_data_location):\n",
    "        for doc in files:\n",
    "            if doc.endswith('.csv'):\n",
    "                for line in csv.DictReader(open(os.path.join(root,doc))):\n",
    "                    outfile.write(line['body_text'] + '\\n')\n",
    "                    sentences.append(line['body_text'])\n",
    "                    cluster_ids.append(int(line['cluster_id']))\n",
    "                outfile.write(doc_separator + '\\n')\n",
    "                cluster_ids.append(-1)\n",
    "cluster_ids = numpy.array(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We combine all documentas with a special character in between string\n",
    "# This cell works if tokenized is in memory\n",
    "\n",
    "separated = sum(zip(tokenized,[[doc_separator]]*len(tokenized)),tuple())\n",
    "sentences = sum(separated,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for root, dirs, files in open(os.walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ad6eaaf6d8aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# wordcount the count of words, ordered by appearance in text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# worddict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mworddict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# wordcount the count of words, ordered by appearance in text\n",
    "# worddict \n",
    "worddict, wordcount = vocab.build_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.save_dictionary(worddict, wordcount, vocab_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters\n",
    "\n",
    "Definitely set:\n",
    "* saveto: a path where the model will be periodically saved\n",
    "* dictionary: where the dictionary is.\n",
    "\n",
    "Consider tuning:\n",
    "* dim_word: the dimensionality of the RNN word embeddings\n",
    "* dim: the size of the hidden state\n",
    "* max_epochs: the total number of training epochs\n",
    "\n",
    "* decay_c: weight decay hyperparameter\n",
    "* grad_clip: gradient clipping hyperparamter\n",
    "* n_words: the size of the decoder vocabulary\n",
    "* maxlen_w: the max number of words per sentence. Sentences longer than this will be ignored\n",
    "* batch_size: size of each training minibatch (roughly)\n",
    "* saveFreq: save the model after this many weight updates\n",
    "\n",
    "Other options:\n",
    "* displayFreq: display progress after this many weight updates\n",
    "* reload_: whether to reload a previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    saveto = model_location,\n",
    "    dictionary = vocab_location,\n",
    "    n_words = 1000,\n",
    "    dim_word = 100,\n",
    "    dim = 500,\n",
    "    max_epochs = 1,\n",
    "    saveFreq = 100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.trainer(sentences,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tools.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tools.encode(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skipthoughts.encode(model,sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate?\n",
    "\n",
    "Supervised task. Apply cluster_id as label to each sentence. Run regression. Evaluate performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regression.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
