{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your own skip-thoughts model\n",
    "\n",
    "This notebook walks you through training a skip-thoughts model. It was used with semi-success to train a skip-thoughts model on a gpu-backed machine with the Pythia kernel against the entire stackexchange corpus. It was prepared over the course of 8 days, and isn't perfect.\n",
    "\n",
    "The first section will require the user to make several data paths.\n",
    "\n",
    "The second section requires importing several modules, including pythia and skip-thoughts-specific modules, which may depend on python paths being set correctly. It may need some tweaking.\n",
    "\n",
    "Since the Jupyter notebook on the gpu-backed machines was unreliable, I moved some sections of this notebook into small dirty python files in a folder called skip-thoughts_training.\n",
    "\n",
    "There is some trickiness about what a skip-thought model is. The model provided by the skip-thoughts paper was actually trained 3 times, once on its corpus, then again on its corpus with half as many dimensions, then again on its corpus with reversed-order sentences, again with half-as many dimensions. They then concatenate the results.\n",
    "\n",
    "We don't attempt this. We merely train a single  \"uni-skip\" model.\n",
    "\n",
    "The encode function found in the file skipthoughts.py insists on two models, uni-skip and bi-skip. To encode off a single uni-skip module, the encode function found in the tools.py file is needed. It unfortunately tends to die and die badly when I use it.\n",
    "\n",
    "The last part of the code is supposed to be validation. Since my encoding is broken, validation is not tested.\n",
    "\n",
    "Because things kept dying on me, I found it convenient to write to disk near constantly. This slows things down, but makes them more robust against dying computers.\n",
    "\n",
    "The cells thus tend to use little memory and instead read and write in a streaming fashion, using a lot of time instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coding data paths\n",
    "\n",
    "Mostly this notebook should just run. It however requires the user to deal with one of the next 2 cells. The following locations will not work out of the box and are just suggestions.\n",
    "\n",
    "#### Location of the data\n",
    "\n",
    "`sample_location` should be the path to a directory which contains your data. Each file should contain json-parsable lines. The directory can have subdirectories. The code will recursively find the files. There should be no `.json` files anywhere in the directory except those the code wishes to parse.\n",
    "\n",
    "`path_to_word2vec` is a `.bin` word2vec file the code depends on, e.g. the Google News model founds at https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "#### Where to put output\n",
    "\n",
    "`parsed_data_location` is a directory of `.csv` files the code will create structured the same as `sample_location`, but where the sentences have been normalized an tokenized, and where each file reprents a post.\n",
    "\n",
    "`training_data_location` is the name of a file that will store the sentences in a single file, one per line, with null characters separating blog posts.\n",
    "\n",
    "`vocab_location` should be the name of a pickle file (including path), which will store information about the words in the corpus\n",
    "\n",
    "`model_location` should be the name of a .npz (zipped numpy) file (including path), which will store the model itself as a numpy array. The code will also create a .npz.pkl file with the same name containing some metadata.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_location = 'pythia/data/stackexchange/all'\n",
    "path_to_word2vec = 'outside-data/stackexchange/models/word2vecAnime.bin'\n",
    "parsed_data_location = 'outside-results/testing'\n",
    "training_data_location = 'outside-results/testing/training.txt'\n",
    "vocab_location = 'outside-data/stackexchange/models/vocab.pickle'\n",
    "model_location = 'outside-data/stackexchange/models/corpus.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's import some modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import auxillary modules\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import csv\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import theano\n",
    "import theano\n",
    "import theano.tensor as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May need to set the flag if your .theanorc isn't correct.\n",
    "# If you want to run on gpu, you should fix your .theanorc\n",
    "# and make this cell irrelevant\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Double check that floatX is float 32\n",
    "# device should be either cpu or gpu, as desired.\n",
    "print(theano.config.floatX)\n",
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this next cell is maybe bad. The notebook only runs if your paths are all configured right. You may need to adjust the below cell to import pythia/skipthoughts models.\n",
    "\n",
    "The commented-out lines were what I used to make this work on my own computer without any adjustments to my notebook kernel. I *hope* this will just work with the pythia kernel installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import skipthoughts modules\n",
    "#sys.path.append('/Users/chrisn/mad-science/pythia/src/featurizers/skipthoughts')\n",
    "#from training import vocab, train, tools\n",
    "#import skipthoughts\n",
    "from src.featurizers.skipthoughts import skipthoughts\n",
    "from src.featurizers.skipthoughts.training import vocab, train, tools\n",
    "# Import pythia modules\n",
    "#sys.path.append('/Users/chrisn/mad-science/pythia/')\n",
    "from src.utils import normalize, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For evaluation purposes, import some sklearn modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Because there were a lot of annoying warnings.\n",
    "# The Beautiful Soup module as used in the pythia normalization is mad about something\n",
    "# And the skip-thoughts code is full of deprecation warnings about how numpy works. The warnings can crash my system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and normalization\n",
    "\n",
    "Who knows the best way to do this? I tried to match the expectations of both the skip-thoughts code and the pythia codebase as best I could.\n",
    "\n",
    "For each document:\n",
    "\n",
    "1) Make list of sentences. We use utils.tokenize.punkt_sentences\n",
    "\n",
    "2) Normalize each sentence. Remove html and make everything lower-case. We use utils.normalize.xml_normalize\n",
    "\n",
    "3) Tokenize each sentence. Now each sentence is a string of space-separated tokens. We use utils.tokenize.word_punct_tokens and rejoin the tokens.\n",
    "\n",
    "Because I had so many difficulties with things crashing, I was happy whenever I got anything done and wanted to save where I was. I also became gunshy about using memory. The solution below is thus entirely streaming. This slows it down because of file i/o.\n",
    "\n",
    "The output of this section run on the entire stackexchange corpus can be found in <...>/stackexchange_models/se_posts_parsed.tar.gz.\n",
    "\n",
    "(Well, the tarring was done in the shell. This cell just creates a directory.)\n",
    "\n",
    "The section requires previously set varaibles:  `sample_location` for the input and `parsed_data_location` for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_extension = \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instead of trying to parse in memory, can instead parse line by line and write to disk\n",
    "fieldnames = [\"body_text\", \"post_id\",\"cluster_id\", \"order\", \"novelty\"]\n",
    "for root,dirs,files in os.walk(sample_location):\n",
    "    for doc in files: \n",
    "        if doc.endswith(file_extension): #Recursively find all .json files\n",
    "            for line in open(os.path.join(sample_location,root,doc)):\n",
    "                temp_dict = json.loads(line)\n",
    "                post_id = temp_dict['post_id']\n",
    "                text = temp_dict['body_text']\n",
    "                sentences = tokenize.punkt_sentences(text)\n",
    "                normal = [normalize.xml_normalize(sentence)\n",
    "                          for sentence in sentences]\n",
    "                tokens = [' '.join(tokenize.word_punct_tokens(sentence))\n",
    "                          for sentence in normal]\n",
    "                base_doc = doc.split('.')[0]\n",
    "                output_filename = \"{}_{}.csv\".format(base_doc,post_id)\n",
    "                # Creates one output file per line of input file.\n",
    "                # Output file includes post id in name:\n",
    "                # {clusterid}_{postid}.csv\n",
    "                rel_path = os.path.relpath(root,sample_location)\n",
    "                output_path = os.path.join(parsed_data_location,\n",
    "                                           rel_path,\n",
    "                                           output_filename)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok = True)\n",
    "                with open(output_path,'w') as token_file:\n",
    "                    #print(parsed_data_location,rel_path,output_filename)\n",
    "                    writer = csv.DictWriter(token_file, fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    output_dict = temp_dict\n",
    "                    for token in tokens:\n",
    "                        output_dict['body_text'] = token\n",
    "                        writer.writerow(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat to match skip-thoughts code input\n",
    "\n",
    "`tokenized` is now a list of lists. Each inner list represents a document as a list of strings, where each string represents a sentence.\n",
    "\n",
    "### An annoying issue\n",
    "\n",
    "The trainer expects a list of sentences. To match expectations, those inner brackets need to disappear.\n",
    "\n",
    "However, this then looks like we have one real long document where the documents have been smashed together in arbitrary order. And the training will mistake the first sentence of one document as being part of the context of the last sentence of another. For sufficiently long documents, you can argue this is just noise. For documents that are themselves only a few sentences, this seems like too much noise.\n",
    "\n",
    "My cludgy fix is to introduce a sentence consisting of a single null character `'\\0'` and add this sentence between every document when concatenating. This may have unintended side-effects.\n",
    "\n",
    "As above, this notebook doesn't depend on much memory. The next cell does not assume you have `tokenized` stored and thus asks you to read it back in. I found this more convenient in the end.\n",
    "\n",
    "The cell depends on previously defined variables `parsed_data_location` and `training_data_location` for input and output respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_separator = '\\0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell does three things\n",
    "# Writes sentences to a text file one line per sentence, with the null character separating documents.\n",
    "# Stores all sentences into a list\n",
    "# Stores the cluster_ids into a numpy array. Each sentence gets the cluster_id of its post. So the list and numpy array\n",
    "# are the same length.\n",
    "sentences = []\n",
    "cluster_ids = []\n",
    "with open(training_data_location,'w') as outfile:\n",
    "    for root, dirs, files in os.walk(parsed_data_location):\n",
    "        for doc in files:\n",
    "            if doc.endswith('.csv'):\n",
    "                for line in csv.DictReader(open(os.path.join(root,doc))):\n",
    "                    outfile.write(line['body_text'] + '\\n')\n",
    "                    sentences.append(line['body_text'])\n",
    "                    cluster_ids.append(int(line['cluster_id']))\n",
    "                outfile.write(doc_separator + '\\n')\n",
    "                cluster_ids.append(-1)\n",
    "cluster_ids = numpy.array(cluster_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the skip-thoughts training dictionaries\n",
    "\n",
    "These are pretty basic things about the whole corpus required by the skip-thoughts code.\n",
    "\n",
    "wordcount is a dictionary of wordcounts, ordered by the order the words appear in the sentences. worddict is a dictionary of the same words, with values corresponding to their rank in the count, ordered by rank in the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can skip this cell if sentences is still in memory\n",
    "sentences = [x.strip() for x in open(training_data_location).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wordcount the count of words, ordered by appearance in text\n",
    "# worddict \n",
    "worddict, wordcount = vocab.build_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.save_dictionary(worddict, wordcount, vocab_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "#### First set parameters\n",
    "\n",
    "Definitely set:\n",
    "* saveto: a path where the model will be periodically saved\n",
    "* dictionary: where the dictionary is.\n",
    "\n",
    "Both these should have been previously set as `model_location` and `vocab_location` respectively.\n",
    "\n",
    "Consider tuning:\n",
    "* dim_word: the dimensionality of the RNN word embeddings (Default 620)\n",
    "* dim: the size of the hidden state (Default 2400)\n",
    "* max_epochs: the total number of training epochs (Default 5)\n",
    "\n",
    "* decay_c: weight decay hyperparameter (Default 0, i.e. ignored)\n",
    "* grad_clip: gradient clipping hyperparamter (Default 5)\n",
    "* n_words: the size of the decoder vocabulary (Default 20000)\n",
    "* maxlen_w: the max number of words per sentence. Sentences longer than this will be ignored (Default 30)\n",
    "* batch_size: size of each training minibatch (roughly) (Default 64)\n",
    "* saveFreq: save the model after this many weight updates (Default 1000)\n",
    "\n",
    "Other options:\n",
    "* displayFreq: display progress after this many weight updates (Default 1)\n",
    "* reload_: whether to reload a previously saved model (Default False)\n",
    "\n",
    "#### Some obvervations on parameters\n",
    "\n",
    "The default displayFreq is 1. Which seems low. It means every iteration prints something. It seems excessive. I suggest 100.\n",
    "\n",
    "As long as the computer can handle it in memory, a bigger batch size seems better all around. I am trying 256.\n",
    "\n",
    "A good chunk of stackexchange sentences seemed to be at least 30 tokens. I am changing that setting to 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using a small set of paramters for testing\n",
    "params = dict(\n",
    "    saveto = model_location,\n",
    "    dictionary = vocab_location,\n",
    "    n_words = 1000,\n",
    "    dim_word = 100,\n",
    "    dim = 500,\n",
    "    max_epochs = 1,\n",
    "    saveFreq = 100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.trainer(sentences,**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding sentences\n",
    "\n",
    "The model created doesn't quite fit into the pipeline, because it is a \"uni-skip\" model, not a \"combine skip\" model. The pipeline uses skipthoughts.encode, which requires very particularly formatted models.\n",
    "\n",
    "The model built above instead works with the encode function found in the tools model.\n",
    "\n",
    "Except that this function often breaks.\n",
    "\n",
    "I have not trained a \"combine-skip model\". The model here is the equivalent of `utable.npy`.\n",
    "\n",
    "One would still need to train an `btable.npy` equivalent. A btable is created by training a model with half the dimension, reversing the sentences and training again, then concatenating the two models into btable. I have not done this and may be missing some subtelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell requires hardcoded paths in tools.py to be changed. It should perhaps also be fixed to not depend\n",
    "# on hardcoded paths.\n",
    "embed_map = tools.load_googlenews_vectors(path_to_word2vec)\n",
    "model = tools.load_model(embed_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Having a lot of trouble getting this line to not crash. It causes a \"floating point exception\".\n",
    "tools.encode(model,sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate?\n",
    "\n",
    "Supervised task. Apply cluster_id as label to each sentence. Run regression. Evaluate performance.\n",
    "\n",
    "Since there is so much stackexchange data, a random sample may be sufficient. So choose a percentage of the data to sample from. That sample will then get divided into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_percent = 1 # Choose a subsample of the data\n",
    "holdout_percent = 0.5  # Of that subsample, make this amount training data\n",
    "                       # and the rest testing data\n",
    "\n",
    "# e.g. 1,000,000 sentences. evaluation_percent = 0.1,\n",
    "# holdout_percent = 0.8 --\n",
    "# Choose 100,000 sentences.\n",
    "# Then choose 80,000 of those for training\n",
    "# and 20,000 of those for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the sentences if they are not already in memory.\n",
    "sentences = [x.strip() for x in open(training_data_location).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in cluster ids, again if not already in memory.\n",
    "cluster_ids = []\n",
    "for root, dirs, files in os.walk(parsed_data_location):\n",
    "    for doc in files:\n",
    "        if doc.endswith('.csv'):\n",
    "            for line in csv.DictReader(open(os.path.join(root,doc))):\n",
    "                cluster_ids.append(int(line['cluster_id']))\n",
    "            cluster_ids.append(-1)\n",
    "cluster_ids = numpy.array(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check. Should be true.\n",
    "num_sentences == len(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample a percentage of your data specified above as evaluation_percent.\n",
    "indices = numpy.arange(num_sentences)\n",
    "num_samples = int(evaluation_percent * num_sentences)\n",
    "index_sample = numpy.sort(numpy.random.choice(indices,\n",
    "                                              size=num_samples,\n",
    "                                              replace = False))\n",
    "sample_sentences = [sentences[i] for i in index_sample]\n",
    "sample_clusters = cluster_ids[index_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Broken!!!\n",
    "# This section requires the encodings of the previous section. But...\n",
    "#encodings = tools.encode(model, sample_sentences)\n",
    "encodings = numpy.random.rand(num_samples,10)\n",
    "#Since I can't get encodings to actually work, have some random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point forward, the code is not well-tested because I couldn't get the encode function to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding_train, encoding_test, cluster_train, cluster_test = train_test_split(encodings,\n",
    "                                                                              sample_clusters,\n",
    "                                                                              test_size = holdout_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(encoding_train, cluster_train)\n",
    "regression.predict(encoding_test)\n",
    "regression.score(encoding_test, cluster_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end.\n",
    "\n",
    "This is the end of the notebook. Below is an alternative approach. Not as well tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An in-memory approach.\n",
    "\n",
    "Because everything kept crashing on me, I ultimately found it most convenient to do everything in a streaming fashion with a lot of writing to disk at every stage. This is obviously slower than desirable. Basically I do a thing, write out the results, read the results back in, then do the next thing.\n",
    "\n",
    "Below is an in-memory approach that reads everything into memory and pushes forward, still sometimes saving key steps to disk, but without any rereading in. Because of various technical issues, this code has never been tested at scale. It works on the anime dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_dicts = [json.loads(line)\n",
    "            for root,dirs,files in os.walk(sample_location)\n",
    "            for doc in files\n",
    "            for line in open(os.path.join(sample_location,root,doc))\n",
    "            ]\n",
    "# doc_dicts is a list of dictionaries, each containing document data\n",
    "# In the anime sample, the text is labeled 'body_text'\n",
    "# There is a field cluster_id which we will use as the categorical label\n",
    "cluster_ids = [d['cluster_id'] for d in doc_dicts]\n",
    "docs = [d['body_text'] for d in doc_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(doc_dicts) # For efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of sentences for each doc\n",
    "sentenced = [tokenize.punkt_sentences(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize each sentence\n",
    "normalized = [[normalize.xml_normalize(sentence) for sentence in doc] for doc in sentenced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(sentenced) #If you're done with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize each sentence\n",
    "tokenized = [[' '.join(tokenize.word_punct_tokens(sentence)) for sentence in doc] for doc in normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separated = sum(zip(tokenized,[[doc_separator]]*len(tokenized)),tuple())\n",
    "sentences = sum(separated,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separated = sum(zip(tokenized,[[doc_separator]]*len(tokenized)),tuple())\n",
    "sentences = sum(separated,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves you with the sentences object in memory, leaving you ready to build the skip-thoughts training dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
